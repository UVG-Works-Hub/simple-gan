{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 6\n",
    "### Deep learning\n",
    "- Daniel Gomez 21429\n",
    "- Samuel Chamale 21881\n",
    "- Adrian Rodriguez 21691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "clpls1FESBFo"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "from IPython import display\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AQxV4Am9nYxa"
   },
   "source": [
    "Datos de entrenamiento\n",
    "\n",
    "1. `(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()`: Carga el conjunto de datos MNIST, dividiéndolo en imágenes y etiquetas de entrenamiento. El segundo conjunto (de prueba) no se utiliza, por eso se ignora con `_`.\n",
    "\n",
    "2. `train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')`: Redimensiona las imágenes de entrenamiento para que tengan un formato de 28x28 píxeles con 1 canal (escala de grises) y convierte los valores a tipo `float32`.\n",
    "\n",
    "3. `train_images = (train_images - 127.5) / 127.5`: Normaliza los valores de los píxeles, que originalmente están entre 0 y 255, para que queden en el rango [-1, 1]. Esto es importante para mejorar el rendimiento de las redes neuronales, especialmente cuando se usan activaciones como `tanh`, que esperan entradas en este rango."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0ge-9IM9SFkW"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "iJWcQPpNSHLA"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcw8hxtmnJR-"
   },
   "source": [
    "Este código crea un conjunto de datos (`train_dataset`) a partir de las imágenes de entrenamiento (`train_images`). Utiliza la función `from_tensor_slices` para convertir el array de imágenes en un formato compatible con TensorFlow, y luego aplica dos operaciones:\n",
    "\n",
    "1. `shuffle(BUFFER_SIZE)`: Desordena aleatoriamente las imágenes utilizando un tamaño de buffer especificado por `BUFFER_SIZE`, lo que ayuda a mezclar los datos antes del entrenamiento.\n",
    "2. `batch(BATCH_SIZE)`: Agrupa las imágenes en lotes del tamaño `BATCH_SIZE`, lo que permite que el modelo entrene en pequeños subconjuntos de datos en cada iteración.\n",
    "\n",
    "Esto organiza las imágenes de entrenamiento para ser usadas de manera eficiente en el proceso de entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "77oc7EKASIXA"
   },
   "outputs": [],
   "source": [
    "# Batch and shuffle the data\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8oehruJmdLn"
   },
   "source": [
    "El generador toma un vector de 100 números aleatorios como entrada y lo transforma en una imagen de 28x28 píxeles. Usa capas densas, BatchNormalizatio), activación LeakyReLU, y varias capas Conv2DTranspose para aumentar el tamaño de la imagen gradualmente hasta obtener la salida final, que tiene una única capa de profundidad y usa la activación tanh para generar la imagen final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ffdqytupSJ_2"
   },
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(7*7*64, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((7, 7, 64)))\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', use_bias=False))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2DTranspose(1, (3, 3), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    },
    "id": "MJohYW14SMGU",
    "outputId": "468d1ea4-a5d5-4edc-8d1e-382397a1d680"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15076d250>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = make_generator_model()\n",
    "noise = tf.random.normal([1, 100])\n",
    "generated_image = generator(noise, training=False)\n",
    "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pzzf-Q7kmz4s"
   },
   "source": [
    "El discriminador toma una imagen de 28x28 píxeles como entrada y la procesa a través de capas convolucionales. Utiliza capas de `Dropout` para prevenir el sobreajuste y reduce la dimensionalidad de la imagen con una capa `Flatten`. Finalmente, una capa densa con una única salida determina si la imagen es real o generada por el modelo, proporcionando una probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "fGO78hK1SPe-"
   },
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ABj-fZWhSRkP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.01711228]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LmD7JhNLST7o"
   },
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4K1k4OrnzPe"
   },
   "source": [
    "1. `real_loss`: Calcula la pérdida del discriminador al clasificar imágenes reales. Compara las salidas del discriminador (`real_output`) con un conjunto de etiquetas de unos (porque las imágenes reales deberían clasificarse como \"reales\"). Se utiliza la función de pérdida de entropía cruzada (`cross_entropy`) para esta comparación.\n",
    "\n",
    "2. `fake_loss`: Calcula la pérdida del discriminador al clasificar imágenes generadas (falsas). Compara las salidas del discriminador (`fake_output`) con etiquetas de ceros (porque las imágenes generadas deberían clasificarse como \"falsas\").\n",
    "\n",
    "3. `total_loss`: Suma las pérdidas de las imágenes reales y generadas para obtener la pérdida total del discriminador.\n",
    "\n",
    "El objetivo es minimizar esta pérdida, de modo que el discriminador mejore en clasificar imágenes reales como reales y falsas como falsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "E8y0mQPKSZ_1"
   },
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfyufbKioFqp"
   },
   "source": [
    "Esta función define la pérdida del generador. El objetivo del generador es crear imágenes falsas que engañen al discriminador haciéndole creer que son reales.\n",
    "\n",
    "1. `fake_output`: Es la salida del discriminador al clasificar las imágenes generadas por el generador.\n",
    "\n",
    "2. `cross_entropy(tf.ones_like(fake_output), fake_output)`: Calcula la pérdida del generador comparando las salidas del discriminador (`fake_output`) con un conjunto de etiquetas de unos (porque el generador quiere que el discriminador clasifique las imágenes falsas como \"reales\"). Se utiliza la función de entropía cruzada para medir qué tan bien está logrando esto.\n",
    "\n",
    "El generador trata de minimizar esta pérdida, de modo que el discriminador no pueda diferenciar entre las imágenes reales y las generadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "u1XHWfcgSbXk"
   },
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "euHXPFL9SdyL"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 8\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-tef-ZZoeCl"
   },
   "source": [
    "La anotación `@tf.function` optimiza la función compilándola para que sea más rápida.\n",
    "\n",
    "1. **Generación de ruido**: Se crea un vector de ruido aleatorio con dimensiones `[BATCH_SIZE, noise_dim]` para alimentar al generador.\n",
    "\n",
    "2. **GradientTape**: `\n",
    "   - Se utilizan dos tf.GradientTape`, uno para el generador (`gen_tape`) y otro para el discriminador (`disc_tape`). Esto permite calcular los gradientes para ambos modelos.\n",
    "   \n",
    "3. **Generación de imágenes**: El generador usa el ruido para generar imágenes falsas (`generated_images`).\n",
    "\n",
    "4. **Evaluación del discriminador**:\n",
    "   - `real_output`: El discriminador procesa las imágenes reales y devuelve una clasificación.\n",
    "   - `fake_output`: El discriminador evalúa las imágenes generadas y también devuelve una clasificación.\n",
    "\n",
    "5. **Cálculo de pérdidas**:\n",
    "   - `gen_loss`: Pérdida del generador, basada en qué tan bien las imágenes generadas engañan al discriminador.\n",
    "   - `disc_loss`: Pérdida del discriminador, basada en su capacidad para diferenciar entre imágenes reales y generadas.\n",
    "\n",
    "6. **Cálculo de gradientes**:\n",
    "   - Se calculan los gradientes de la pérdida con respecto a los pesos entrenables del generador y el discriminador.\n",
    "\n",
    "7. **Actualización de los modelos**:\n",
    "   - `generator_optimizer.apply_gradients`: Aplica los gradientes calculados para actualizar los pesos del generador.\n",
    "   - `discriminator_optimizer.apply_gradients`: Hace lo mismo para el discriminador.\n",
    "\n",
    "En resumen, este bloque realiza un paso completo de entrenamiento, ajustando tanto el generador como el discriminador para que ambos mejoren en sus respectivos objetivos (engañar o no ser engañado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "OkEsWa2FSfNi"
   },
   "outputs": [],
   "source": [
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "2A97SYzaSg7B"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input):\n",
    "  # Notice `training` is set to False.\n",
    "  # This is so all layers run in inference mode (batchnorm).\n",
    "  predictions = model(test_input, training=False)\n",
    "\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
    "      plt.axis('off')\n",
    "\n",
    "  # plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhrwIJ_opDYO"
   },
   "source": [
    "\n",
    "1. **Inicio del bucle de épocas**:\n",
    "   - El entrenamiento se repite por la cantidad de épocas definida en el argumento `epochs`.\n",
    "   - Al inicio de cada época, se registra el tiempo con `start = time.time()` para medir cuánto tarda cada una.\n",
    "\n",
    "2. **Entrenamiento por lotes**:\n",
    "   - Dentro de cada época, se itera sobre cada lote de imágenes del conjunto de datos (`dataset`), llamando a la función `train_step(image_batch)` para entrenar el generador y discriminador en ese lote.\n",
    "\n",
    "3. **Generación de imágenes**:\n",
    "   - Se genera y guarda una imagen utilizando la función `generate_and_save_images(generator, epoch + 1, seed)`, donde el generador crea nuevas imágenes a partir de una semilla (`seed`), permitiendo un seguimiento visual del progreso de la GAN.\n",
    "\n",
    "4. **Medición del tiempo**:\n",
    "   - Se calcula y muestra el tiempo que tomó completar cada época con `print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))`.\n",
    "\n",
    "5. **Generación final**:\n",
    "   - Después de la última época, se genera una imagen final con el mismo método de visualización.\n",
    "\n",
    "En resumen, esta función coordina el entrenamiento de la GAN durante múltiples épocas, mide el tiempo por cada una y genera imágenes en cada iteración para ver cómo mejora la calidad de las imágenes generadas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar modelo en pickle para evitar generarlo desde 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save models\n",
    "def save_models(generator, discriminator, filename='gan_models.pickle'):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump((generator.get_weights(), discriminator.get_weights()), f)\n",
    "\n",
    "# Function to load models\n",
    "def load_models(filename='gan_models.pickle'):\n",
    "    if os.path.exists(filename):\n",
    "        with open(filename, 'rb') as f:\n",
    "            gen_weights, disc_weights = pickle.load(f)\n",
    "        \n",
    "        generator = make_generator_model()\n",
    "        discriminator = make_discriminator_model()\n",
    "        \n",
    "        generator.set_weights(gen_weights)\n",
    "        discriminator.set_weights(disc_weights)\n",
    "        \n",
    "        return generator, discriminator\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "X0e6OQrhSjl5"
   },
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "    generator, discriminator = load_models()\n",
    "    \n",
    "    if generator is None or discriminator is None:\n",
    "        print(\"No saved models found. Training from scratch.\")\n",
    "        generator = make_generator_model()\n",
    "        discriminator = make_discriminator_model()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            for image_batch in dataset:\n",
    "                train_step(image_batch)\n",
    "\n",
    "            print(f'Time for epoch {epoch + 1} is {time.time() - start} sec')\n",
    "\n",
    "        # Save the models after training\n",
    "        save_models(generator, discriminator)\n",
    "    else:\n",
    "        print(\"Loaded pre-trained models.\")\n",
    "\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "a4TIZzd7SlXI",
    "outputId": "607a8537-038e-4829-d5fc-f5e9348c7729"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved models found. Training from scratch.\n",
      "Time for epoch 1 is 8.942246913909912 sec\n",
      "Time for epoch 2 is 7.949536085128784 sec\n",
      "Time for epoch 3 is 7.877950191497803 sec\n",
      "Time for epoch 4 is 7.87118673324585 sec\n",
      "Time for epoch 5 is 7.590961933135986 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 00:14:23.587297: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 6 is 7.79961895942688 sec\n",
      "Time for epoch 7 is 7.764219284057617 sec\n",
      "Time for epoch 8 is 7.550108194351196 sec\n",
      "Time for epoch 9 is 7.61547589302063 sec\n",
      "Time for epoch 10 is 7.760595321655273 sec\n",
      "Time for epoch 11 is 7.6766510009765625 sec\n",
      "Time for epoch 12 is 7.744585990905762 sec\n",
      "Time for epoch 13 is 7.845700025558472 sec\n",
      "Time for epoch 14 is 7.6882781982421875 sec\n",
      "Time for epoch 15 is 7.758831977844238 sec\n",
      "Time for epoch 16 is 7.80100679397583 sec\n",
      "Time for epoch 17 is 7.807690858840942 sec\n",
      "Time for epoch 18 is 7.821901082992554 sec\n",
      "Time for epoch 19 is 7.86004900932312 sec\n",
      "Time for epoch 20 is 7.841271877288818 sec\n",
      "Time for epoch 21 is 7.769537925720215 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 00:16:27.764014: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 22 is 7.870103120803833 sec\n",
      "Time for epoch 23 is 7.801549911499023 sec\n",
      "Time for epoch 24 is 7.802904844284058 sec\n",
      "Time for epoch 25 is 7.810208082199097 sec\n",
      "Time for epoch 26 is 7.868797302246094 sec\n",
      "Time for epoch 27 is 7.8158018589019775 sec\n",
      "Time for epoch 28 is 7.821940898895264 sec\n",
      "Time for epoch 29 is 7.972002983093262 sec\n",
      "Time for epoch 30 is 7.881480932235718 sec\n",
      "Time for epoch 31 is 7.940237998962402 sec\n",
      "Time for epoch 32 is 7.949548006057739 sec\n",
      "Time for epoch 33 is 7.921467065811157 sec\n",
      "Time for epoch 34 is 7.937698125839233 sec\n",
      "Time for epoch 35 is 7.957895040512085 sec\n",
      "Time for epoch 36 is 7.851471900939941 sec\n",
      "Time for epoch 37 is 7.8766868114471436 sec\n",
      "Time for epoch 38 is 7.782449722290039 sec\n",
      "Time for epoch 39 is 8.093356847763062 sec\n",
      "Time for epoch 40 is 8.021322965621948 sec\n",
      "Time for epoch 41 is 8.036663055419922 sec\n",
      "Time for epoch 42 is 7.75915002822876 sec\n",
      "Time for epoch 43 is 7.871958017349243 sec\n",
      "Time for epoch 44 is 7.940787076950073 sec\n",
      "Time for epoch 45 is 8.329031944274902 sec\n",
      "Time for epoch 46 is 8.731348991394043 sec\n",
      "Time for epoch 47 is 7.847253084182739 sec\n",
      "Time for epoch 48 is 7.833235025405884 sec\n",
      "Time for epoch 49 is 8.004571914672852 sec\n",
      "Time for epoch 50 is 7.897706031799316 sec\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the MNIST dataset\n",
    "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "train_images = (train_images - 127.5) / 127.5\n",
    "\n",
    "BUFFER_SIZE = 60000\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "EPOCHS = 50\n",
    "noise_dim = 100\n",
    "\n",
    "generator, discriminator = train(train_dataset, EPOCHS)\n",
    "\n",
    "# Generate and save a sample image\n",
    "noise = tf.random.normal([1, noise_dim])\n",
    "generated_image = generator(noise, training=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M6Yx4aNZR735"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
